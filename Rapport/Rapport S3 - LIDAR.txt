
Rapport de projet
Le LIDAR









Remerciements : 

Je tenais à remercier toute l’équipe enseignante sans qui ce projet n’aurait pu voir le jour. 

Et en particulier, M. Guinand qui a su transmettre parfaitement ses connaissances et ses compétences durant tout le semestre. J’ai acquis de nouveaux savoirs en réalisant un démonstrateur de projet base mobile, de la programmation d’application en python et dans le traitement de donnée lié à un lidar. 
Je souhaite aussi adresser mes remerciements à Mme Docarmo qui m’a accompagné et conseillée dans la rédaction de ce rapport.
Et enfin je remercie M. Ardillier pour l’aide apportée à mon projet sur la conception de ma carte électronique.


Table des matières
I.	Le LIDAR	8
1.	Etude de faisabilité	8
a.	La problématique	8
b.	Les solutions envisageables	8
c.	L’avantage de la Raspberry PI	9
d.	Le langage de programmation	9
e.	Le fonctionnement d'un LIDAR	10
f.	Le schéma synoptique et le logigramme	10
g.	Le planning	11
2.	Le traitement des données	12
a.	La bibliothèque compatible avec le lidar	12
b.	Programme de test du lidar	13
c.	Programme de traitement des données du lidar	15
d.	Unité et saturation	16
e.	Exploitation des données traitées	16
f.	Programme de simulation	18
II.	La Raspberry pi	19
1.	La configuration	19
2.	L’interface graphique	19
3.	L’extension CAN	19
III.	Le démonstrateur	20
1.	Le rôle du démonstrateur	20



Introduction :
La robotique est une plateforme fascinante pour explorer les frontières de l'innovation et de la technologie. Dans le cadre de la 31e édition des Rencontres de Robotique, notre équipe le CRAC (Club de Robotique des Amis de la Coupe) s'est engagée à préparer la voie vers l'infini en se concentrant sur notre destination première, Mars. Toutefois, une question vitale se pose : comment garantir la subsistance de notre équipage lors de cette mission interplanétaire ? C'est là que nos robots entrent en scène, avec pour objectif essentielle de préparer le terrain et de constituer des réserves alimentaires.
Notre tâche se décline en plusieurs volets indépendantes, chacun représentant un défi unique pour nos robots. Du rempotage des plantes à l'orientation des panneaux solaires, en passant par la pollinisation et le rechargement des batteries, chaque action joue un rôle capital dans la réussite de notre expédition.

Accès au Code Source sur GitHub
Le code source du projet est disponible sur GitHub, offrant une transparence totale et la possibilité pour d'autres étudiants de reproduire mon travail. Suivez les étapes ci-dessous pour accéder au code source :
* GitHub Repository: https://github.com/quent1-lab/CRAC-2024

Clonage du Répertoire :
Pour obtenir une copie locale du code, utilisez la commande Git suivante dans votre terminal :
* Commande bash : « git clone https://github.com/quent1-lab/CRAC-2024 »

Structure du Projet :
Voici une brève explication de la structure des répertoires :
* [Lidar] : Dossier comprenant les programmes pythons utile au fonctionnement du lidar
* [Lidarobot] : Dossier comprenant le projet « platformio » du démonstrateur
* [CAN] : Dossier comprenant les programmes correspondant à la communication CAN

Exécution du Code :
Pour exécuter le code, assurez-vous de suivre les instructions spécifiques du fichier README.md situé à la racine du répertoire. De plus, une image de l’OS Raspberry Pi est présente avec les bonnes versions des logiciels utilisés

Conseil : N'hésitez pas à ouvrir des issues sur GitHub en cas de questions ou de problèmes rencontrés lors de la reproduction du projet.

Présentation de la Coupe :
La 31e édition des Rencontres de Robotique se déroule dans un environnement complexe représentant une serre autonome sur Mars. L'aire de jeu, un plan rectangulaire est parsemé de divers éléments tels que des plantes, des panneaux solaires, des pots en fer, des jardinières, des coccinelles robotiques et des zones de départ.

Figure 1 : Figure des divers éléments présent sur le terrain

Missions principales
* Rempoter les plantes et les mettre en culture : Les robots doivent transférer les plantes nouvellement arrivées de la Terre dans des pots adaptés et les placer dans des environnements de culture spécifiques pour améliorer leur productivité. Différents types de plantes ont des exigences environnementales spécifiques.

* Orienter les panneaux solaires : Pour assurer un approvisionnement énergétique optimal, les robots doivent orienter les panneaux solaires vers le côté de la table associé à leur équipe.

* Assurer la pollinisation des plantes : Les robots doivent relâcher des coccinelles (Petits Actionneurs Mobiles Indépendants - PAMI) pour polliniser les plantes. Les coccinelles doivent atteindre des plantes ou des pots spécifiques.

* Retourner se recharger les batteries : Après avoir accompli leurs tâches, les robots doivent retourner dans leurs aires de recharge respectives pour recharger leurs batteries. Cependant, ils ne peuvent pas utiliser les zones de départ comme points de recharge.

* Anticiper le futur rendement de la récolte : Les équipes doivent estimer le nombre de points qu'elles pensent réaliser pendant le match. Cette estimation peut être affichée de manière statique avant le match ou de manière dynamique pendant le match. Un bonus est accordé en fonction de la précision de l'estimation.


Aire de jeu
L'aire de jeu est un plan rectangulaire de 3000 mm par 2000 mm avec des bordures de 70 mm de haut et 22 mm d'épaisseur. 

Figure 2 :  Vue générale de l’aire de jeu
Les points
Chaque mission accomplie rapporte des points, mais des contraintes spécifiques doivent être respectées pour que les actions soient valides. Les points estimés en fin de match peuvent[QC1] également contribuer à un bonus, l’objectif est de déterminer le nombre de points récoltés sur une partie automatiquement et de vérifier si ce score est juste.

Les robots
Chaque équipe peut homologuer un robot principal et, éventuellement, un ou plusieurs PAMIs (Petits Actionneurs Mobiles Indépendants), avec des contraintes dimensionnelles spécifiques. Pour garantir une distinction claire entre les équipes, les robots et les PAMIs doivent être reconnaissables depuis le public. 
Les dimensions des robots respectent des règles strictes, avec un périmètre maximal de 1200 mm au départ et de 1300 mm lorsqu'ils sont totalement déployés au cours du match. De plus, la hauteur du robot et des objets manipulés ne doit pas dépasser 350 mm pendant le match, à l'exception du bouton d'arrêt d'urgence, autorisé à atteindre 375 mm.

Figure 3 : Dimensions d'un robot

I. Le LIDAR
1. Etude de faisabilité 
a. La problématique
La difficulté du projet réside dans la détection précise et efficace des robot[QC2]s adverses au sein de l'aire de jeu. La capacité à localiser avec précision ces robots constitue un élément déterminant pour la réussite des différentes missions assignées à notre équipe. Sans une détection adéquate, notre robot risque de ne pas réagir de manière appropriée aux mouvements des adversaires, compromettant ainsi la réalisation des objectifs.

Le lidar sur lequel l’étude est portée à pour référence : « RPLidar S1 »

Figure 4 : Illustration RPLidar S1

b. Les solutions envisageables
Dans la phase initiale du projet, j’ai envisagé deux solutions principales pour le traitement des données du lidar : l'utilisation d'une ESP32 ou l'utilisation d'une Raspberry Pi.
- Solution 1 : ESP32
L'ESP32, est un microcontrôleur ‘’puissant’’, et est une option à considérer pour traiter les données du lidar. Sa taille compacte et son faible coût en font un choix attractif. Cependant, sa capacité de traitement de données limité et sa capacité de stockage restreinte m’ont alarmé quant à sa capacité à gérer les tâches complexes associées au traitement du LIDAR et à la communication des informations en temps réel.
- Solution 2 : Raspberry Pi
La Raspberry Pi étant que micro-ordinateur, offre une puissance de calcul plus importante, une capacité de stockage étendue, et la possibilité d'utiliser des langages de programmation plus avancés. Cela le positionne comme une option plus robuste pour traiter les données en temps réel, avec la flexibilité nécessaire pour évoluer en fonction des exigences du projet.


c. L’avantage de la Raspberry PI
La Raspberry Pi a été sélectionnée comme la solution privilégiée pour plusieurs raisons :
- Puissance de Calcul
La Raspberry Pi offre une puissance de calcul significativement supérieure à l'ESP32, permettant un traitement plus rapide et plus efficace des données du lidar. Cela s'avère crucial dans un environnement en temps réel où des décisions rapides et précises sont nécessaires.
- Flexibilité et Langages de Programmation
La Raspberry Pi offre une flexibilité en termes de langages de programmation. Cela permettra à l’équipe d'implémenter des algorithmes de traitement de données plus avancés en utilisant des langages tels que Python ou le langage C, offrant ainsi une plus grande souplesse dans le développement du logiciel de contrôle du robot.
- Évolutivité
La nature évolutive de la Raspberry Pi offre la possibilité d'ajouter des fonctionnalités supplémentaires à l'avenir sans compromettre les performances. Cela permettra d'adapter le robot à des missions plus complexes ou à des environnements changeants.
En conclusion, la Raspberry Pi s'avérée être la solution idéale pour le traitement des données du lidar dans le cadre du projet de robotique. Sa puissance, sa capacité de stockage et sa flexibilité vont permettent d'optimiser le fonctionnement du robot, offrant ainsi une réponse plus efficace aux défis rencontrés sur le terrain de la coupe.
d. Le langage de programmation
Le choix du langage de programmation revêt une importance capitale dans le traitement des données du lidar. Python a été sélectionné en raison de ses caractéristiques intrinsèques, particulièrement adaptées aux exigences du projet.
Python se distingue par sa syntaxe claire et sa simplicité, rendant le code facile à lire et à comprendre. Cette qualité est cruciale dans un projet complexe, où la collaboration entre membres de l'équipe peut grandement bénéficier d'une compréhension aisée du code. De plus, il est facile de déployer un programme python sur tous les OS existants.
Python offre un écosystème de bibliothèques riche et varié, notamment des bibliothèques spécialisées dans le traitement de données scientifiques et la manipulation de données spatiales. La communauté Python est dynamique et engagée, offrant un support continu ainsi que des ressources abondantes.
Python facilite la mise en œuvre du multitâche grâce à des modules tels que "multiprocessing ". Cette caractéristique permet à plusieurs tâches d’être exécutées simultanément, comme la navigation en fonction des données du lidar, et la communication avec d'autres systèmes.
La flexibilité de Python se révèle également dans l'intégration de communication sans fil. La gestion du Wifi, du Bluetooth et du bus CAN est simplifiée, permettant une communication fluide entre le robot et d'autres dispositifs.
e. Le fonctionnement d'un LIDAR
Le lidar, acronyme de "Light Detection and Ranging," est un dispositif de télédétection qui utilise des faisceaux laser pour mesurer la distance entre l'émetteur-récepteur du lidar et une surface cible. Son principe de fonctionnement repose sur l'émission et la réception de pulsation lumineuse de type laser vers l'environnement, suivie de la mesure du temps écoulé entre l'émission et la réception du signal réfléchi. La vitesse de la lumière permet de calculer la distance avec une précision remarquable.
Le dispositif effectue un balayage rotatif, créant ainsi une image en deux dimensions de la topographie environnante. Les points capturés représentent les coordonnées spatiales et les distances dans le plan horizontal, fournissant une représentation précise des obstacles et de la structure à proximité.

Figure 5 : Illustration du fonctionnement d'un capteur LIDAR
En robotique, le lidar est souvent utilisé pour la perception de l'environnement, la navigation autonome, et la détection d'obstacles. La technologie lidar est indispensable dans divers domaines, allant des applications industrielles à la conduite autonome, offrant une perception précise et en temps réel de l'espace environnant.
f. Le schéma synoptique et le logigramme
La conception du système repose sur la clarté[QC3] des différentes tâches à accomplir. Le schéma synoptique offre une vision d'ensemble, illustrant les principaux composants et leur interrelation.

Le lidar, monté sur le robot, émet des faisceaux laser qui interagissent avec l'environnement, générant ainsi des données brutes. Ces données sont transmises au Raspberry Pi, où le programme de traitement s'engage. Une fois les données traitées, le Raspberry Pi envoie des ordres aux autres systèmes du robot via un bus CAN.

Le logigramme détaille ensuite le flux logique du programme. Après l'initialisation de la connexion, le moteur du lidar est activé, déclenchant le processus de balayage. Le traitement des données intervient par la suite tout en s’adaptant et prenant en compte les composantes externes.



Figure 6 : Schéma synoptique du projet lidar

Figure 7 : Logigramme représentant le fonctionnement du projet lidar
g. Le planning
Le projet lidar a été lancé en septembre avec une vision des étapes à franchir, reflétée dans un diagramme de Gantt détaillé. Chaque phase planifiée, a contribué à la réalisation du lidar, des premières étapes de recherche et de conception jusqu'à la phase de développement et de tests. Les jalons ont été respectées, et le projet atteindra sa conclusion début janvier. 

Figure 8 : Diagramme de Gantt du projet Lidar
2. Le traitement des données
Le développement du logiciel pour l’intégration sur le robot se réalise sur deux supports. Dans un premier temps, l’élaboration d’un programme fiable en phase de debugge sur un ordinateur. Puis l’intégration du système sur le robot grâce à la Raspberry Pi.
a. La bibliothèque compatible avec le lidar
Pour le traitement des données du lidar, j’ai exploité la bibliothèque "rplidar" qui propose une solution simple pour interagir avec les capteurs de télémétrie RPLidar offrant une communication efficace et précise grâce à ses fonctions intégrées. Cependant, quelques adaptations ont été nécessaires en raison de son origine conçue pour des versions plus récentes que celle du matériel sur le projet.

- Vitesse de communication
La connexion se fait via une communication série, avec deux options de vitesse de communication : 115 200 ou 256 000 bauds. Lors de l’initialisation de la connexion, la valeur par défaut de la bibliothèque est de 115 200. Or après plusieurs tests, il s’est avéré que cette vitesse n’était pas compatible avec l’installation sans pouvoir en déterminer clairement la cause. Il a fallu modifier la vitesse de communication à 256 000.

Figure 9 : Image de l'adaptateur du lidar montrant les vitesses en bauds
Figure 10 : Lignes de code de l'initialisation de la classe RPLidar dans rplidar.py
- Démarrage du moteur
Des erreurs sont survenues lors de l'exécution d'un programme de test, indiquant un problème de déchiffrement de la trame « Descriptor length mismatch[QC4] ». Ces problèmes étaient liés à la fonction de démarrage du moteur, initialement configurée pour des modèles de lidar plus récents.
Pour résoudre ce problème, la commande « SCAN_BYTE » doit être envoyé au lidar. Cette commande permet de démarrer le moteur, ainsi que la mesure de l’environnement.

Figure 11 : Ligne de code corresponde à l'ordre de lancement du scan


Cette solution généré une nouvelle anomalie, le programme cherchant à récupérer trop rapidement les données du lidar afin de les traitées sans que le moteur ait atteint sa vitesse nominale, aucunes données n’étaient envoyées et le programme s’arrêter brusquement. Pour cela, il fallut simplement ajouter une temporisation de 2 secondes avant de traiter les données.

Figure 12 : Lignes de code de la fonction de lancement du scan
- Appel de la fonction « start_motor »
Cette fonction ne fait pas que démarrer le moteur mais lance aussi le scan, il a fallu définir le bon endroit pour l’appeler. Dans le cas où cette fonction est appelée juste après la connexion à l’ordinateur, des erreurs vont survenir. Par exemple, si la fonction permettant de récupérer l’état de santé du lidar est appelé. Car, en utilisant la commande « SCAN_BYTE », le lidar renvoie directement les données brutes des mesures et n’attend pas de recevoir d’autre ordre que de s’arrêter.
Le meilleur endroit où appeler cette fonction, est dans la fonction « iter_measurments », pendant l’initialisation, juste avant la boucle principale. 
Une fois tous ces points modifier, toutes les conditions sont réunies pour que les données du lidar soient récupérées et traitées.
Toutes fois, il est possible que le lidar ne s’arrête pas correctement en fin de programme, ou qu’une anomalie apparaisse lors de la connexion série entre le lidar et l’ordinateur. Ces erreurs devront être détecter dans le programme principal par l’utilisation de la méthode python « try » et de la classe «RPLidarException» de la bibliothèque « rplidar » qui hérite de la méthode python « Exception ». Une fois utilisé dans le programme principal permettra de levée les exceptions liées à l’usage du lidar sans interrompre le fonctionnement.

Figure 13 : Lignes de code de la classe RPLidarException
b. Programme de test du lidar
Afin de vérifier le bon fonctionnement des modifications apportées, le lidar a été testé avec un programme basique. Ce dernier créer le lien entre le lidar et l’ordinateur, puis lance le scan de la pièce en affichant les points mesurés. Ce programme d’exemple « TestLidar.py » permet de comprendre la façon dont les données d’angle et de distance sont récupéré pour être traité. Ce programme fonctionne aussi bien sur Linux que sur Windows, il suffit d’indiquer le bon port USB correspondant au lidar[QC5]. Une autre méthode de connexion permet plus facilement de se connecter via le numéro de série du port du lidar. Pour cette méthode, il faut s’assurer qu’aucun autre appareil soit branché à l’ordinateur (Bluetooth compris) avant de l’exécuter, et enfin de noter le numéro de série affiché sur le terminal.

Figure 14 : Lignes de code correspondant au choix du port du lidar
Ensuite, on retrouve dans le programme la boucle de récupération des mesures. La distance mesurée est remplie dans un tableau « scan_data » de 360 cases, chaque case correspondant à l’angle de mesure. La fonction « min ([359, floor(angle)]) », permet de ne jamais dépasser la taille du tableau. 
Enfin, le tableau de mesure est donné en argument à la fonction « process_data », qui permet d’afficher les points mesurés grâce à une bibliothèque graphique.

Figure 15 : Lignes de code de la boucle de récupération des données du lidar
En observant les images ci-dessous, on remarque qu’avec une simple installation du lidar dans une pièce, on obtient 360 points représentant la pièce scannée avec les meubles.


Figure 16 : Premier test du lidar avec affichage

Figure 17 : Capture d'écran de la fenêtre d'affichage des mesures du lidar

c. Programme de traitement des données du lidar
Une fois l’acquisition des points obtenu, le traitement des données fiabilise la localisation des obstacles dans toutes les circonstances. Que ça soit dans la robustesse du programme, dans son intégration sur Raspberry pi ou encore dans la pertinence des données traitées.
Lorsque le robot circule sur l’aire de jeu, le lidar se déplace avec lui, et donc les points mesurés se « déplace » aussi. Pour cela, avant de d’exploiter les données, il faut transformer chaque point mesuré afin de compenser le déplacement du robot pour qu’un point immobile sur l’aire de jeu garde les mêmes coordonnées par exemple.

Figure 18 : Illustration de détection obstacle sans transformation des coordonnées
Sur l’illustration ci-dessus, on remarque dans le cas où le robot s’est déplacé, les coordonnées de l’objets détecté ne sont pas les mêmes que l’objets réel qui lui est resté immobile.

Pour résoudre ce problème, tous les points mesurés vont donc être repositionné en fonction de l’emplacement du robot sur le l’aire jeu. Cette étape consiste à calculer une position (x,y) d’un point grâce à sa distance et son angle en fonction des coordonnées ( x, y, thêta ) du robot[QC6].

Figure 19 : Lignes de code représentant le calcul des nouvelles coordonnées d'un point de mesure
Dans ce code, le nouvelle angle calculé est ramené dans l'intervalle [0, 360] afin de calculer les coordonnées du point.
d. Unité et saturation
Concernant l'unité de mesure, le programme est configuré pour fonctionner en millimètres (mm), assurant ainsi une précision fine des mesures. Cette unité a été sélectionnée en tenant compte de la résolution du lidar.
En termes de saturation des points, la conception du programme prend en compte la taille du terrain comme limite. La saturation est étroitement liée à la zone d'opération du robot, garantissant que seuls les objets situés à l'intérieur du périmètre défini par le terrain sont pris en compte. Cette approche est essentielle pour éviter que le robot ne détecte des objets situés à l'extérieur du champ opérationnel, minimisant ainsi les risques d'interférences inutiles. De plus, la saturation est définie avec une marge de sécurité tenant compte de la taille du robot. Ainsi, le programme optimise la détection d'obstacles, assurant une réactivité adéquate tout en limitant les fausses alertes.

Figure 20 : Illustration de la saturation de détection d'obstacle

e. Exploitation des données traitées
Chaque objet détecté est représenté par une instance dans la classe « Objet », caractérisée par son identifiant unique, ses coordonnées « (x, y) », sa taille et des informations sur son mouvement.
La fonction « detect_object » parcourt les scans afin d’identifier les objets présents dans l'environnement. Elle prend en compte la proximité des points détectés, exclut ceux déjà associés à des objets connus, et calcule les coordonnées moyennes pondérées des points autour de chaque objet. De plus, elle détermine la taille des objets en fonction des écarts entre les points.
La gestion des objets est effectuée dans une liste, permettant ainsi le suivi dynamique d'objets au fil du temps. Chaque objet est mis à jour avec sa nouvelle position, taille, direction et vitesse lors de chaque itération. Cette approche offre une représentation en temps réel des objets détectés.
En complément des fonctionnalités de suivi d'objets, le programme intègre une anticipation des trajectoires des robots sur le terrain. La méthode « trajectoires_anticipation » permet de simuler les déplacements futurs du robot actuel et du robot adverse, ainsi que de générer une trajectoire d'évitement en cas de collision potentielle.
La simulation utilise les positions et vitesses actuelles des robots pour anticiper leurs déplacements sur une durée spécifiée (duree_anticipation). À chaque pas de temps (pas_temps), les nouvelles positions sont calculées, et les trajectoires des robots sont mises à jour.
Si la distance entre les robots devient inférieure à une valeur critique (distance_securite), indiquant un risque de collision, le programme propose une trajectoire d'évitement. 

Figure 21 : Représentation de l'exploitation des données
Actuellement, toutes les fonctions d’affichage sont intégrées dans le programme de traitement des données afin de faciliter le développement du projet. A termes, le fonctionnement des programmes se feront en multitâches et sur différents appareils. Cela permettra d’augmenter la capacité de calcul sur la Raspberry pi en déportant la supervision.

f. Programme de simulation
La fonction « programme_simulation[QC7] » joue un rôle essentiel dans le développement et le test du système de détection et de suivi d'objets du robot. Grâce à la fonction « valeur_de_test » des valeurs simulé du lidar permettent de le remplacer en attribuant des distances aléatoires à des angles spécifiques. Ces valeurs servent de données d'entrée pour évaluer la performance du système sans la nécessité d'un environnement physique.
Le programme de simulation reproduit quasiment à l’identique le fonctionnement réel. En utilisant la classe ComESP32, il peut également communiquer avec un microcontrôleur ESP32, afin de tester la récupération des données du démonstrateur. L’interface graphique est la même qu’en usage classique et affiche[QC8] le robot, les objets détectés, ainsi que les trajectoires anticipées du robot en fonction des objets environnants.
L'interaction clavier permet de déplacer le robot virtuel, ce qui facilite le test de l'algorithme de détection et d'évitement d'obstacles dans divers scénarios. Cette simulation en temps réel offre un moyen pratique de vérifier la robustesse du système, d'ajuster les paramètres, et d'optimiser les algorithmes avant le déploiement sur le robot physique. 

Figure 22 : Représentation du programme de simulation
g. L’interface graphique
L'emploi de PyGame pour l'interface graphique a simplifié le développement du logiciel. PyGame, est une bibliothèque de développement de jeux en Python, permettant la création d’une interface utilisateur interactive et intuitive. Les fonctionnalités graphiques de PyGame ont permis de l’intégrer plus facilement au projet.


II. La Raspberry pi
La Raspberry Pi constitue un élément essentiel du projet, nécessitant une configuration minutieuse et une interface graphique adaptée à ses différentes phases d'utilisation.
1. La configuration
La configuration de la Raspberry Pi est une étape fondamentale pour assurer son bon fonctionnement. L’idéal est d'installer le système d'exploitation optimisé pour la Raspberry Pi. La procédure standard inclut le téléchargement de l'image, son installation sur une carte micro-SD, et l'initialisation du système. Cette étape est simplifiée par l’usage du logiciel « Raspberry Pi Imager ». L’OS recommandé est « Raspberry Pi OS (64-bit). Mais il est possible de choisir la version serveur, afin de retirer l’interface graphique et ainsi économiser les ressources de la machine.
Il est également crucial de configurer les paramètres réseau d’avance dans les options avancées du logiciel, assurant un accès à distance par défaut. De plus, une fois l’installation terminé, il est conseillé d’activer dans les paramètres de l’OS les options suivantes : Serial ; Wire ; VNC.

2. L’interface graphique
L'interface graphique de la Raspberry Pi joue un rôle crucial dans les phases de développement. Durant cette période, une interface graphique pratique et fonctionnelle facilite la programmation, le débogage, et la surveillance en temps réel. Cependant, une fois le programme développé et intégré au robot, l'interface graphique sur la Raspberry Pi devient superflue. Dans cette configuration, il est judicieux de déporter l'interface graphique sur un autre appareil, permettant une gestion à distance plus efficace, tout en libérant les ressources de la Raspberry Pi pour des tâches plus spécifiques à la robotique. Ce déplacement stratégique améliore les performances globales du système.

3. L’extension CAN
Etant donné que l’ensemble des systèmes du robot communique via un bus CAN, il était nécessaire d’ajouter une extension CAN à la Raspberry Pi. Le module « RS485 CAN HAT » permet de rajouter une communication CAN à la Raspberry Pi. Un code d’exemple est présent dans le projet Github. De plus, une petite carte électronique à été rajouté afin de rendre compatible la connectique avec le système existant.

Figure 23 : Module RS485 CAN HAT
Figure 24 : PCB de l'adaptateur du bus CAN
Figure 25 : schéma électrique de l'adaptateur du bus CAN

III. Le démonstrateur
1. Le rôle du démonstrateur
Le démonstrateur occupe une place importante dans le développement et la validation des fonctions du lidar, notamment en lien avec les mécanismes d'odométrie de la base roulante. Sa conception vise à mettre en œuvre et à tester les algorithmes intégrés au programme au logiciel du lidar. Grâce à cette approche, le démonstrateur permet de valider l'efficacité et la précision des méthodes utilisées, contribuant ainsi à l'optimisation des performances globales du système.

La base roulante a été choisi sur une base Pololu, avec des motoréducteurs encodeurs. Ce sont des moteurs continus ayant une réduction de 1 :100 et un encodeur intégré. Ce matériel a été choisi après divers test et recherche, afin de correspondre au mieux aux besoins.
De plus, les fichiers 3D modélisés pour le démonstrateur sont mis à disposition dans le projet GitHub. Le programme C++ associé au démonstrateur est également accessible sur la plateforme GitHub, fournissant des bibliothèques complètes, écrites et testées pour l'utilisation des moteurs et des encodeurs.

Cependant, des améliorations peuvent être apportées, notamment en fabriquant une nouvelle carte électronique prenant mieux en compte les demandes énergétiques de la base roulante et de la Raspberry Pi, ainsi que dans l'ergonomie globale.


Figure 26 : Démonstrateur du projet lidar


Conclusion

En conclusion de ce projet, qui a représenté un investissement significatif sur une période de plusieurs mois, je retire une richesse d'enseignements issus des multiples défis et expériences rencontrés tout au long du processus. La gestion globale du projet, la recherche active de solutions et la résolution de problèmes variés ont constitué une plateforme d'apprentissage exceptionnelle. 
Ce parcours m’a permis d'acquérir un éventail de connaissances telles que :
- La collaboration efficace en équipe ;
- La conception de cartes électroniques ;
- Une maîtrise accrue des langages Arduino/C++ et Python ;
- L’affinement de compétences dans l'utilisation des logiciels Fusion360 et SolidWorks.

Table des figures
Figure 1 : Figure des divers éléments présent sur le terrain	6
Figure 2 :  Vue générale de l’aire de jeu	7
Figure 3 : Dimensions d'un robot	7
Figure 4 : Illustration RPLidar S1	8
Figure 5 : Illustration du fonctionnement d'un capteur LIDAR	10
Figure 6 : Schéma synoptique du projet lidar	11
Figure 7 : Logigramme représentant le fonctionnement du projet lidar	11
Figure 8 : Diagramme de Gant du projet Lidar	11
Figure 9 : Image de l'adaptateur du lidar montrant les vitesses en bauds	12
Figure 10 : Lignes de code de l'initialisation de la class RPLidar dans rplidar.py	12
Figure 11 : Ligne de code corresponde à l'ordre de lancement du scan	12
Figure 12 : Lignes de code de la fonction de lancement du scan	13
Figure 13 : Lignes de code de la class RPLidarException	13
Figure 14 : Lignes de code correspondant au choix du port du lidar	14
Figure 15 : Lignes de code de la boucle de récupération des données du lidar	14
Figure 16 : Premier test du lidar avec affichage	14
Figure 17 : Capture d'écran de la fenêtre d'affichage des mesures du lidar	14
Figure 18 : Illustration de détection obstacle sans transformation des coordonnées	15
Figure 19 : Lignes de code représentant le calcul des nouvelles coordonnées d'un point de mesure	15
Figure 20 : Illustration de la saturation de détection d'obstacle	16
Figure 21 : Représentation de l'exploitation des données	17
Figure 22 : Représentation du programme de simulation	18
Figure 23 : Module RS485 CAN HAT	19
Figure 24 : Démonstrateur du projet lidar	20



Annexes
[QC1]A developper
[QC2]Trouver le nom du type de robot
[QC3]A revoir
[QC4]Décrire
[QC5]Ajouter l’explication en annexion
[QC6]Trop d’angle a revoir
[QC7]Nom de la fonction
[QC8]Mode debug ?
Professeur référent : Yves Guinand
Année : 2023 – 2024

Quentin Cunha – groupe A1	Page 5 sur 40

Étudiants :Professeur référent :Quentin CUNHA - Groupe A1Yves Guinand

